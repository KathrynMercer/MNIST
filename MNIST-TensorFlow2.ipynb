{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "3659ebb1-18c5-4498-98b8-16e0e2767247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds #tensorflow_datasets has a lot of data sets for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90613ed8-1ad0-4fc1-a086-8c087d8dc19d",
   "metadata": {},
   "source": [
    "## Preprocessing - load, shuffle, assign validation data, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "f85a8f4c-6ed3-40df-a09b-ccaa3afe8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "mnist_dataset, mnist_info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "c6f13564-2619-4383-acff-a0d4503ea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into testing & training data sets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "a6f2eebc-8f16-4d71-b892-91cee17b7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a validation data set\n",
    "# figuring out how large the validation series will be based on converting 10% of the test data to validation\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64) #make sure the sample count is an int\n",
    "\n",
    "# figuring out how large the test data sample actually is (for convenience)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "64c536aa-32d2-4299-a102-0d74d52e6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data. (I could have also called tf's map function)\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255. #. continues the float use\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "389b8847-da57-4082-92f5-6bdf48856d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_and_validation_data = mnist_train.map(scale) #scales the entire train data set\n",
    "scaled_test_data = mnist_test.map(scale) #scale the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "e7ff1e58-b062-4563-990e-a5639f99f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling to make sure batching doesn't have confounding patterns (ex. by date)\n",
    "BUFFER_SIZE = 10000 #make sure we don't try to shuffle everything at once if the data set is massive\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "c14d6212-a783-4219-9bf8-dbb26bd62b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the validation data from the combined data set after shuffling\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "00af9633-3d26-4b0e-8743-5fdb89522eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "train_data = train_data.batch(BATCH_SIZE) #adds a column to the tensor indicating batches\n",
    "# the validation & test data doesn't have to be batched to save computational power since they are only used for\n",
    "# forward propagation, but the model will expect batching, so we will make one batch w/ all data\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = scaled_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "614042f5-5d05-4bf3-8d47-de608a7f353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make inputs & targets iterable (necessary since imported as tuple w/ supervision) and loads first batch\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0348dd6-a30f-42ff-a737-b61c7648775f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "7eafb3c4-dc36-45e1-8b9a-c3d7a33d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlining the model w/ hyperparamters\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 500 #guesswork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "c9471003-47a7-4df8-8f7f-99830e1c604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape= (28,28,1)), #each image is 28pixels x x28 x1\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'), # make 1st hidden layer, adjusted activation function based on training data (relu seems to consistently outperform the other options for this data set)\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'), # repeating hidden layers\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax') # make output layer, using softmax since this will assign probability for classification data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "9bcb9b37-6156-4296-b724-a466d1ee9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimizer & the loss function\n",
    "# sparse applies one-hot encoding without an additional step\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf047b3b-06be-4912-95a3-da884fcd9688",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "9d844a36-edbb-4a68-81af-16b17e338b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 - 1s - 27ms/step - accuracy: 0.8609 - loss: 0.4775 - val_accuracy: 0.9473 - val_loss: 0.1843\n",
      "Epoch 2/10\n",
      "54/54 - 1s - 14ms/step - accuracy: 0.9624 - loss: 0.1283 - val_accuracy: 0.9653 - val_loss: 0.1172\n",
      "Epoch 3/10\n",
      "54/54 - 1s - 14ms/step - accuracy: 0.9746 - loss: 0.0837 - val_accuracy: 0.9758 - val_loss: 0.0825\n",
      "Epoch 4/10\n",
      "54/54 - 1s - 13ms/step - accuracy: 0.9832 - loss: 0.0548 - val_accuracy: 0.9828 - val_loss: 0.0588\n",
      "Epoch 5/10\n",
      "54/54 - 1s - 14ms/step - accuracy: 0.9889 - loss: 0.0376 - val_accuracy: 0.9838 - val_loss: 0.0520\n",
      "Epoch 6/10\n",
      "54/54 - 1s - 13ms/step - accuracy: 0.9904 - loss: 0.0311 - val_accuracy: 0.9860 - val_loss: 0.0455\n",
      "Epoch 7/10\n",
      "54/54 - 1s - 14ms/step - accuracy: 0.9927 - loss: 0.0241 - val_accuracy: 0.9893 - val_loss: 0.0352\n",
      "Epoch 8/10\n",
      "54/54 - 1s - 13ms/step - accuracy: 0.9943 - loss: 0.0180 - val_accuracy: 0.9900 - val_loss: 0.0337\n",
      "Epoch 9/10\n",
      "54/54 - 1s - 14ms/step - accuracy: 0.9942 - loss: 0.0176 - val_accuracy: 0.9920 - val_loss: 0.0253\n",
      "Epoch 10/10\n",
      "54/54 - 1s - 14ms/step - accuracy: 0.9957 - loss: 0.0131 - val_accuracy: 0.9925 - val_loss: 0.0237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x183ec993e30>"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 10 #arbitrary choice to start\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebdb347-ffaa-4163-9bf8-070ddbf9dc61",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "c3f7196a-29db-4dac-aa0b-0004365bb7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9775 - loss: 0.0876\n",
      "Test loss: 0.09 Test accuracy: 97.75%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f} Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e9f69-1f80-46ea-ae89-ee2382ab26de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensor Flow Environment",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
